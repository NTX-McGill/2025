{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precompute CLIP embeddings on the THINGS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxence/dev/python/NTX/ntx_venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open_clip\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxence/dev/python/NTX/ntx_venv/lib/python3.12/site-packages/open_clip/factory.py:380: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): GELU(approximate='none')\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"ViT-L-14\"\n",
    "PRETRAINED_DATASET = \"openai\"  # Use \"laion2b_s32b_b82k\" for OpenCLIP versions\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess, tokenizer = open_clip.create_model_and_transforms(MODEL_NAME, pretrained=PRETRAINED_DATASET)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA\n",
    "The _image_database_things dataset can be downloaded on the OSF page of the THINGS dataset: https://osf.io/jum2f/\n",
    "It is around 4.7GB compressed and 5.1GB uncompressed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"./_image_database_things/object_images\"\n",
    "OUTPUT_FILE = \"reduced_things_clip_embeddings.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROCESS ALL IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing categories: 1855it [00:31, 58.22it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1854 embeddings\n",
      "Saved 1854 embeddings to reduced_things_clip_embeddings.pt\n",
      "The file is 6.01 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "embeddings = {}\n",
    "\n",
    "for root, _, files in tqdm(os.walk(IMAGE_DIR), desc=\"Processing categories\", total=len(os.listdir(IMAGE_DIR))):\n",
    "    image_paths = [os.path.join(root, file) for file in files if file.endswith((\".jpg\", \".jpeg\", \".png\"))][:1]\n",
    "\n",
    "    # process batches\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "\n",
    "        images = []\n",
    "        for img_path in batch_paths:\n",
    "            try:\n",
    "                image = Image.open(img_path).convert(\"RGB\")\n",
    "                image = preprocess(image)\n",
    "                images.append(image)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "        images = torch.stack(images).to(device)\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model.encode_image(images).cpu()\n",
    "        \n",
    "        for j, img_path in enumerate(batch_paths):\n",
    "            category = os.path.basename(root)\n",
    "            embeddings[\"/\".join(img_path.split(\"/\")[-2:])] = batch_embeddings[j].unsqueeze(0)\n",
    "\n",
    "print(f\"There are {len(embeddings)} embeddings\")\n",
    "torch.save(embeddings, OUTPUT_FILE)\n",
    "print(f\"Saved {len(embeddings)} embeddings to {OUTPUT_FILE}\")\n",
    "print(f\"The file is {os.path.getsize(OUTPUT_FILE) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VERIFYING EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['photo_booth/photo_booth_09s.jpg', 'photo_booth/photo_booth_02s.jpg', 'photo_booth/photo_booth_12s.jpg', 'photo_booth/photo_booth_11s.jpg', 'photo_booth/photo_booth_03s.jpg', 'photo_booth/photo_booth_13s.jpg', 'photo_booth/photo_booth_05s.jpg', 'photo_booth/photo_booth_06s.jpg', 'photo_booth/photo_booth_10s.jpg', 'photo_booth/photo_booth_15s.jpg']\n",
      "Embeddings have a shape of torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = torch.load(OUTPUT_FILE)\n",
    "print(list(embeddings.keys())[:10])\n",
    "print(\"Embeddings have a shape of\", embeddings['photo_booth/photo_booth_09s.jpg'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ntx_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
